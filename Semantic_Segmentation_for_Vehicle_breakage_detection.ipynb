{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmswl0707/Semantic-Segmentation-for-Vehicle-breakage-detection/blob/main/Semantic_Segmentation_for_Vehicle_breakage_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkLvy-hKG-VA"
      },
      "source": [
        "### Drive mount and package import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2PPlRpblu2z"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hADtuqTop7Wz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.transforms.functional as f\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E738F-qolu24"
      },
      "outputs": [],
      "source": [
        "# data 경로 설정 \n",
        "root = os.path.join(os.getcwd(), \"drive\", \"MyDrive\", \"Colab Notebooks\",\"scratch_small\")\n",
        "root"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amNT58iAS434"
      },
      "source": [
        "### Dataset 준비\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ndo5JDuS436"
      },
      "outputs": [],
      "source": [
        "class SOCAR_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"masks\"))))\n",
        "        \n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        # load images ad masks\n",
        "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"masks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        mask = np.array(mask)[:,:,0]      # 3차원으로 구성된 mask 를 label 로 쓰기 위해 변환\n",
        "\n",
        "        mask[mask > 0] = 1\n",
        "\n",
        "        # there is only one class\n",
        "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
        "\n",
        "        target = {}\n",
        "        target[\"masks\"] = mask\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUatveWKuvsg"
      },
      "source": [
        "### Data augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcr7b9ON9v2x"
      },
      "outputs": [],
      "source": [
        "class RandomHorizontalFlip(object):\n",
        "    def __init__(self, prob):\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < self.prob:\n",
        "            height, width = image.shape[-2:]\n",
        "            image = image.flip(-1)\n",
        "\n",
        "            # 파이토치 argumentation에서 타겟값이 변하지 않으므로, 위치가 바뀔 때 따로 설정\n",
        "            if \"masks\" in target:\n",
        "                target[\"masks\"] = target[\"masks\"].flip(-1)\n",
        "        return image, target\n",
        "'''\n",
        "class RandomRotation(object):\n",
        "    def __init__(self, prob):\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < self.prob:\n",
        "            image = f.rotate(image, angle = self.prob)\n",
        "            if \"masks\" in target:\n",
        "                target[\"masks\"] = f.rotate(target[\"masks\"], angle = self.prob)\n",
        "        return image, target\n",
        "\n",
        "class AdjustBrightness(object):\n",
        "    def __init__(self, prob):\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < self.prob:\n",
        "            brightness_factor=1+(np.random.rand()*2-1)*self.prob\n",
        "            image=f.adjust_brightness(image,brightness_factor)\n",
        "\n",
        "        return image, target\n",
        "'''\n",
        "class ToTensor(object):\n",
        "    def __call__(self, image, target):\n",
        "        image = transforms.ToTensor()(image)\n",
        "        return image, target\n",
        "\n",
        "class Resize(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        image = transforms.Resize(self.size)(image)\n",
        "        if \"masks\" in target:\n",
        "            target[\"masks\"] = transforms.Resize(self.size)(target[\"masks\"].unsqueeze(dim=0)).squeeze()\n",
        "        return image, target\n",
        "\n",
        "class Normalize(object):\n",
        "    def __call__(self, image, target):\n",
        "        image = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(image)\n",
        "        return image, target\n",
        "\n",
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrSKFXDHBA9t"
      },
      "outputs": [],
      "source": [
        "def get_transform(train):\n",
        "    transforms = [ToTensor(), Resize((300,300)), Normalize()]\n",
        "    if train:\n",
        "        transforms.append(RandomHorizontalFlip(0.5))\n",
        "        #transforms.append(RandomRotation(0.2)),\n",
        "        #transforms.append(AdjustBrightness(0.2)),\n",
        "    return Compose(transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wheHUeO2u3I2"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEsXhMPH4eRr",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 데이터 셋이 적을 경우, 사전 학습된 모델을 이용하는 것이 성능에 효과적\n",
        "\n",
        "seg_model = models.segmentation.deeplabv3_resnet50(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 수정하기\n",
        "\n",
        "seg_model.classifier[4]=nn.Conv2d(256, 2, kernel_size=(1,1), stride=(1,1))\n",
        "#seg_model"
      ],
      "metadata": {
        "id": "0qCu4uGAYQm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uVxrVTMvFii"
      },
      "source": [
        "### Dataset split, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94bE6US1w6M4"
      },
      "outputs": [],
      "source": [
        "dent_train = SOCAR_Dataset(os.path.join(root,'train'), get_transform(train=True))\n",
        "dent_valid = SOCAR_Dataset(os.path.join(root,'valid'), get_transform(train=False))\n",
        "dent_test = SOCAR_Dataset(os.path.join(root,'test'), get_transform(train=False))\n",
        "\n",
        "# 메모리 부족으로 배치사이즈 수정은 불가해보임 ㅜㅜ...\n",
        "train_loader = DataLoader(dent_train, batch_size=12, shuffle=True, drop_last=True)\n",
        "valid_loader = DataLoader(dent_valid, batch_size=12, shuffle=False, drop_last=True)\n",
        "test_loader = DataLoader(dent_test, batch_size=2, shuffle=False, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36oCr-8bvQnV"
      },
      "source": [
        "### Trainer class 정의"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scheduler 설정\n",
        "\n",
        "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
        "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
        "        if T_0 <= 0 or not isinstance(T_0, int):\n",
        "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
        "        if T_mult < 1 or not isinstance(T_mult, int):\n",
        "            raise ValueError(\"Expected integer T_mult >= squarepad_visual, but got {}\".format(T_mult))\n",
        "        if T_up < 0 or not isinstance(T_up, int):\n",
        "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
        "        self.T_0 = T_0\n",
        "        self.T_mult = T_mult\n",
        "        self.base_eta_max = eta_max\n",
        "        self.eta_max = eta_max\n",
        "        self.T_up = T_up\n",
        "        self.T_i = T_0\n",
        "        self.gamma = gamma\n",
        "        self.cycle = 0\n",
        "        self.T_cur = last_epoch\n",
        "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.T_cur == -1:\n",
        "            return self.base_lrs\n",
        "        elif self.T_cur < self.T_up:\n",
        "            return [(self.eta_max - base_lr) * self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            return [base_lr + (self.eta_max - base_lr) * (\n",
        "                        1 + math.cos(math.pi * (self.T_cur - self.T_up) / (self.T_i - self.T_up))) / 2\n",
        "                    for base_lr in self.base_lrs]\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        if epoch is None:\n",
        "            epoch = self.last_epoch + 1\n",
        "            self.T_cur = self.T_cur + 1\n",
        "            if self.T_cur >= self.T_i:\n",
        "                self.cycle += 1\n",
        "                self.T_cur = self.T_cur - self.T_i\n",
        "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
        "        else:\n",
        "            if epoch >= self.T_0:\n",
        "                if self.T_mult == 1:\n",
        "                    self.T_cur = epoch % self.T_0\n",
        "                    self.cycle = epoch // self.T_0\n",
        "                else:\n",
        "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
        "                    self.cycle = n\n",
        "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
        "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
        "            else:\n",
        "                self.T_i = self.T_0\n",
        "                self.T_cur = epoch\n",
        "\n",
        "        self.eta_max = self.base_eta_max * (self.gamma ** self.cycle)\n",
        "        self.last_epoch = math.floor(epoch)\n",
        "\n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
        "            param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "n4W-k5ew8nY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"주어진 patience 이후로 validation loss가 개선되지 않으면 학습을 조기 중지\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path= path):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): validation loss가 개선된 후 기다리는 기간\n",
        "                            Default: 7\n",
        "            verbose (bool): True일 경우 각 validation loss의 개선 사항 메세지 출력\n",
        "                            Default: False\n",
        "            delta (float): 개선되었다고 인정되는 monitered quantity의 최소 변화\n",
        "                            Default: 0\n",
        "            path (str): checkpoint저장 경로\n",
        "                            Default: 'None.pt'\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''validation loss가 감소하면 모델을 저장한다.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "metadata": {
        "id": "UnHkG_5W_1e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbGXbbNHK1iU"
      },
      "outputs": [],
      "source": [
        "## Trainer class 정의\n",
        "\n",
        "class Semantic_Seg_Trainer(nn.Module):\n",
        "    def __init__(self, model,opt=\"adam\", num_class=2, lr=0.0001, has_scheduler=False, device=\"cpu\", log_dir=\"./logs\", max_epoch=20):\n",
        "        \"\"\"\n",
        "          Args:\n",
        "            model: 사용할 model\n",
        "            opt: optimizer\n",
        "            lr: learning rate\n",
        "            has_scheduler: learning rate scheduler 사용 여부\n",
        "            device: 사용할 device (cpu/cuda)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        self.max_epoch = max_epoch\n",
        "        self.model = model                            \n",
        "        self.class_weight = torch.FloatTensor(Args['weight']).to('cuda')\n",
        "        self.loss = nn.CrossEntropyLoss(weight=self.class_weight, reduction='mean')\n",
        "        self.num_class = num_class\n",
        "\n",
        "        self._get_optimizer(opt=opt.lower(), lr=lr)   # optimizer 정의\n",
        "        self.has_scheduler = has_scheduler            # scheduler 사용여부 \n",
        "        if self.has_scheduler:\n",
        "            self._get_scheduler()\n",
        "\n",
        "        self.device = device                          # 사용할 device\n",
        "        \n",
        "        self.log_dir = log_dir\n",
        "        if not os.path.exists(log_dir): os.makedirs(log_dir)\n",
        "\n",
        "    def _get_optimizer(self, opt, lr=0.001):\n",
        "        \"\"\"\n",
        "          Args:\n",
        "            opt: optimizer\n",
        "            lr: learning rate\n",
        "        \"\"\"\n",
        "        if opt == \"sgd\":\n",
        "            self.optimizer = torch.optim.SGD(params=self.model.parameters(), lr=lr)\n",
        "        elif opt == \"adam\":\n",
        "            self.optimizer = torch.optim.Adam(params=self.model.parameters(), lr=lr , weight_decay=Args[\"weight_decay\"])\n",
        "        else:\n",
        "            raise ValueError(f\"optimizer {opt} is not supproted\")\n",
        "\n",
        "    def _get_scheduler(self):\n",
        "        #self.scheduler = torch.optim.lr_scheduler.StepLR(optimizer=self.optimizer, step_size=5, gamma=0.5, verbose=True)\n",
        "        #self.scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=self.optimizer, lr_lambda=lambda epoch: 0.85**epoch)\n",
        "        self.scheduler = CosineAnnealingWarmUpRestarts(optimizer=self.optimizer, T_0=10, T_mult=1, eta_max=Args[\"eta_min\"], T_up=2, gamma=0.3)\n",
        "\n",
        "    def train(self, train_loader, valid_loader, max_epochs=20, disp_epoch=1, visualize=False):\n",
        "        \"\"\"\n",
        "          네트워크를 학습시키는 함수\n",
        "          Args:\n",
        "            train_loader: 학습에 사용할 train dataloader\n",
        "            valid_loader: validation에 사용할 dataloader\n",
        "            max_epochs: 학습을 진행할 총 epoch 수\n",
        "            disp_epochs: 학습 log를 display 할 epoch 주기\n",
        "            visualize: 학습 진행 과정에서 결과 이미지를 visualize \n",
        "        \"\"\"\n",
        "        print(\"===== Train Start =====\")\n",
        "\n",
        "        early_stopping = EarlyStopping(patience=Args['patience'], verbose=True)\n",
        "\n",
        "        start_time = time.time()   \n",
        "        history = {\"train_loss\": [], \"valid_loss\": [], \"train_miou\": [], \"valid_miou\": []}\n",
        "        \n",
        "        for e in range(max_epochs):\n",
        "            print(f\"Start Train Epoch {e}\")\n",
        "            train_loss, train_miou = self._train_epoch(train_loader)\n",
        "            print(f\"Start Valid Epoch {e}\")\n",
        "            valid_loss, valid_miou = self._valid_epoch(valid_loader)\n",
        "            \n",
        "            \n",
        "            history[\"train_loss\"].append(train_loss)      # 현재 epoch에서 성능을 history dict에 저장\n",
        "            history[\"valid_loss\"].append(valid_loss)      #\n",
        "            \n",
        "            history[\"train_miou\"].append(train_miou)      # \n",
        "            history[\"valid_miou\"].append(valid_miou)      #\n",
        "\n",
        "            if self.has_scheduler:         # scheduler 사용할 경우 step size 조절\n",
        "                self.scheduler.step()\n",
        "\n",
        "            if e % disp_epoch == 0:        # disp_epoch 마다 결과값 출력 \n",
        "                print(f\"Epoch: {e}, train loss: {train_loss:>6f}, valid loss: {valid_loss:>6f}, train miou: {train_miou:>6f}, valid miou: {valid_miou:>6f}, time: {time.time()-start_time:>3f}\")\n",
        "                start_time = time.time()   \n",
        "\n",
        "            self.save_statedict(save_name=f\"log_epoch_{e}\")\n",
        "            self.plot_history(history, save_name=f\"{self.log_dir}/log_epoch_{e}.png\")       # 그래프 출력\n",
        "\n",
        "    def _train_epoch(self, train_loader, disp_step=10):\n",
        "        \"\"\"\n",
        "          model를 training set 한 epoch 만큼 학습시키는 함수\n",
        "          Args:\n",
        "            train_loader: 학습에 사용할 train dataloader\n",
        "          Returns:\n",
        "            training set 한 epoch의 평균 loss, 평균 accuracy\n",
        "        \"\"\"\n",
        "        epoch_loss = 0\n",
        "        \n",
        "        miou = 0\n",
        "        ious = np.zeros([2])\n",
        "        \n",
        "        self.model.train()           \n",
        "        cnt = 0\n",
        "        epoch_start_time = time.time()\n",
        "        start_time = time.time()\n",
        "        for (x, y) in train_loader:        # x: data, y:label\n",
        "            cnt += 1\n",
        "\n",
        "            x = x.to(self.device)\n",
        "            label = y['masks'].to(self.device).type(torch.long)\n",
        "            \n",
        "            out = self.model(x)              # model이 예측한 output\n",
        "            loss = self.loss(out['out'], label)       \n",
        "\n",
        "            self.optimizer.zero_grad()       # backwardpass를 통한 network parameter 업데이트\n",
        "            loss.backward()                  # \n",
        "            self.optimizer.step()            # \n",
        "        \n",
        "            epoch_loss += loss.to(\"cpu\").item()    \n",
        "            \n",
        "            out_background = torch.argmin(out['out'].to(\"cpu\"), dim=1).to(self.device)           # meanIoU 계산을 위한 데이터 변형\n",
        "            out_target = torch.argmax(out['out'].to(\"cpu\"), dim=1).to(self.device)               #\n",
        "            \n",
        "            ious[0] += self.batch_segmentation_iou(out_background, torch.logical_not(label).type(torch.long)) # ious[0]:background IoU\n",
        "            ious[1] += self.batch_segmentation_iou(out_target, label)                                         # ious[1]:파손 IoU\n",
        "            \n",
        "            if cnt % disp_step == 0:\n",
        "                iou_back = ious[0]/(cnt*x.shape[0])\n",
        "                iou_scratch = ious[1]/(cnt*x.shape[0])\n",
        "                miou = (ious[0]/(cnt*x.shape[0]) + ious[1]/(cnt*x.shape[0])) / 2.\n",
        "                \n",
        "                print(f\"Iter: {cnt}/{len(train_loader)}, train epcoh loss: {epoch_loss/(cnt):>6f}, miou: {miou:>6f}, iou_back : {iou_back:>6f}, iou_scratch : {iou_scratch:>6f}, time: {time.time()-start_time:>3f}\")\n",
        "                start_time = time.time()   \n",
        "\n",
        "        epoch_loss /= len(train_loader)  \n",
        "        \n",
        "        \n",
        "        iou_back = ious[0]/(cnt*x.shape[0])\n",
        "        iou_scratch = ious[1]/(cnt*x.shape[0])\n",
        "        epoch_miou = (ious[0]/(cnt*x.shape[0]) + ious[1]/(cnt*x.shape[0])) / 2.\n",
        "        print(f\"Train loss: {epoch_loss:>6f}, miou: {epoch_miou:>6f}, iou_back : {iou_back:>6f}, iou_scratch : {iou_scratch:>6f}, time: {time.time()-epoch_start_time:>3f}\")\n",
        "\n",
        "        return epoch_loss, epoch_miou\n",
        "  \n",
        "    def _valid_epoch(self, valid_loader, disp_step=10):\n",
        "        \"\"\"\n",
        "          현재 model의 성능을 validation set에서 측정하는 함수\n",
        "          Args:\n",
        "            valid_loader: 학습에 사용할 valid dataloader\n",
        "          Returns:\n",
        "            validation set 의 평균 loss, 평균 accuracy\n",
        "        \"\"\"\n",
        "        epoch_loss = 0\n",
        "        \n",
        "        miou = 0\n",
        "        ious = np.zeros([2])\n",
        "                      \n",
        "        self.model.eval()                 \n",
        "        cnt = 0\n",
        "        epoch_start_time = time.time()\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():          \n",
        "            for (x, y) in valid_loader:\n",
        "                cnt += 1\n",
        "                x = x.to(self.device)\n",
        "                label = y['masks'].to(self.device).type(torch.long)\n",
        "\n",
        "                out = self.model(x) \n",
        "                loss = self.loss(out['out'], label)\n",
        "                      \n",
        "                epoch_loss += loss.to(\"cpu\").item()\n",
        "                \n",
        "                out_background = torch.argmin(out['out'].to(\"cpu\"), dim=1).to(self.device)\n",
        "                out_target = torch.argmax(out['out'].to(\"cpu\"), dim=1).to(self.device)\n",
        "\n",
        "                ious[0] += self.batch_segmentation_iou(out_background, torch.logical_not(label).type(torch.long))\n",
        "                ious[1] += self.batch_segmentation_iou(out_target, label)\n",
        "                \n",
        "                #early_stopping(epoch_loss, self.model)\n",
        "                '''\n",
        "                if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "                '''\n",
        "   \n",
        "                if cnt % disp_step == 0:\n",
        "                    iou_back = ious[0]/(cnt*x.shape[0])\n",
        "                    iou_scratch = ious[1]/(cnt*x.shape[0])\n",
        "                    miou = (ious[0]/(cnt*x.shape[0]) + ious[1]/(cnt*x.shape[0])) / 2.\n",
        "                    print(f\"Iter: {cnt}/{len(valid_loader)}, valid epcoh loss: {epoch_loss/(cnt):>6f}, miou: {miou:>6f}, iou_back : {iou_back:>6f}, iou_scratch : {iou_scratch:>6f}, time: {time.time()-start_time:>3f}\")\n",
        "                    start_time = time.time()   \n",
        "\n",
        "        epoch_loss /= len(valid_loader)\n",
        "        \n",
        "        iou_back = ious[0]/(cnt*x.shape[0])\n",
        "        iou_scratch = ious[1]/(cnt*x.shape[0])\n",
        "        epoch_miou = (ious[0]/(cnt*x.shape[0]) + ious[1]/(cnt*x.shape[0])) / 2.\n",
        "        print(f\"Valid loss: {epoch_loss:>6f}, miou: {epoch_miou:>6f}, iou_back : {iou_back:>6f}, iou_scratch : {iou_scratch:>6f}, time: {time.time()-epoch_start_time:>3f}\")\n",
        "\n",
        "        return epoch_loss, epoch_miou\n",
        "\n",
        "    def save_statedict(self, save_name = None):\n",
        "        \n",
        "        if not save_name == None:\n",
        "            torch.save(seg_model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/pth_path/\"+ save_name +\".pth\")\n",
        "\n",
        "    def plot_history(self, history, save_name=None):\n",
        "        \"\"\"\n",
        "          history에 저장된 model의 성능을 graph로 plot\n",
        "          Args:\n",
        "            history: dictionary with keys {\"train_loss\",\"valid_loss\",  }\n",
        "                     각 item 들은 epoch 단위의 성능 history의 list\n",
        "        \"\"\"\n",
        "        fig = plt.figure(figsize=(16, 8))\n",
        "        \n",
        "        \n",
        "        ax = fig.add_subplot(1, 2, 1)\n",
        "        ax.plot(history[\"train_loss\"], color=\"red\", label=\"train loss\")\n",
        "        ax.plot(history[\"valid_loss\"], color=\"blue\", label=\"valid loss\")\n",
        "        ax.title.set_text(\"Loss\")\n",
        "        ax.legend()\n",
        "        \n",
        "        ax = fig.add_subplot(1, 2, 2)\n",
        "        ax.plot(history[\"train_miou\"], color=\"red\", label=\"train miou\")\n",
        "        ax.plot(history[\"valid_miou\"], color=\"blue\", label=\"valid miou\")\n",
        "        ax.title.set_text(\"miou\")\n",
        "        ax.legend()\n",
        "\n",
        "        plt.show()\n",
        "                      \n",
        "        if not save_name == None:     # graph 저장\n",
        "            plt.savefig(save_name)\n",
        "                      \n",
        "        \n",
        "\n",
        "    def test(self, test_loader):\n",
        "        \"\"\"\n",
        "          현재 model의 성능을 test set에서 측정하는 함수\n",
        "          Args:\n",
        "            test_loader: 학습에 사용할 test dataloader\n",
        "          Returns:\n",
        "            test set 의 평균 loss, 평균 accuracy\n",
        "        \"\"\"\n",
        "        print(\"===== Test Start =====\")\n",
        "        start_time = time.time()\n",
        "        epoch_loss = 0\n",
        "        \n",
        "        miou = 0\n",
        "        ious = np.zeros([2])\n",
        "                      \n",
        "        self.model.eval()                 \n",
        "        cnt = 0\n",
        "        epoch_start_time = time.time()\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():            \n",
        "            for (x, y) in test_loader:\n",
        "                cnt += 1\n",
        "                x = x.to(self.device)\n",
        "                label = y['masks'].to(self.device).type(torch.long)\n",
        "\n",
        "                out = self.model(x) \n",
        "                loss = self.loss(out['out'], label)\n",
        "\n",
        "                epoch_loss += loss.to(\"cpu\").item()\n",
        "                      \n",
        "                out_background = torch.argmin(out['out'].to(\"cpu\"), dim=1).to(self.device)\n",
        "                out_target = torch.argmax(out['out'].to(\"cpu\"), dim=1).to(self.device)\n",
        "\n",
        "                ious[0] += self.batch_segmentation_iou(out_background, torch.logical_not(label).type(torch.long))\n",
        "                ious[1] += self.batch_segmentation_iou(out_target, label)\n",
        "                \n",
        "                if cnt % 10 == 0:\n",
        "                    iou_back = ious[0]/(cnt*x.shape[0])\n",
        "                    iou_scratch = ious[1]/(cnt*x.shape[0])\n",
        "                    miou = (ious[0]/(cnt*x.shape[0]) + ious[1]/(cnt*x.shape[0])) / 2.\n",
        "                    print(f\"Iter: {cnt}/{len(valid_loader)}, test epcoh loss: {epoch_loss/(cnt):>6f}, miou: {miou:>6f}, iou_back : {iou_back:>6f}, iou_scratch : {iou_scratch:>6f}, time: {time.time()-start_time:>3f}\")\n",
        "                    start_time = time.time()  \n",
        "\n",
        "        epoch_loss /= len(test_loader)\n",
        "        \n",
        "        \n",
        "        iou_back = ious[0]/(cnt*x.shape[0])\n",
        "        iou_scratch = ious[1]/(cnt*x.shape[0])\n",
        "        epoch_miou = (ious[0]/(cnt*x.shape[0]) + ious[1]/(cnt*x.shape[0])) / 2.\n",
        "        \n",
        "        print(f\"Test loss: {epoch_loss:>6f}, miou: {epoch_miou:>6f}, iou_back : {iou_back:>6f}, iou_scratch : {iou_scratch:>6f}, time: {time.time()-epoch_start_time:>3f}\")\n",
        "\n",
        "    \n",
        "    def batch_segmentation_iou(self, outputs, labels):\n",
        "        \"\"\"\n",
        "            outputs, labels : (batch, h, w)\n",
        "        \"\"\"\n",
        "        \n",
        "        SMOOTH = 1e-6\n",
        "\n",
        "        intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
        "        union = (outputs | labels).float().sum((1, 2))         # Will be zero if both are 0\n",
        "\n",
        "        iou = (intersection + SMOOTH) / (union + SMOOTH) # union = A+b - intersection\n",
        "            \n",
        "        return torch.sum(iou).to(\"cpu\").numpy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0QATJWUvVys"
      },
      "source": [
        "### Fine-tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Args = {\"lr\" : 0, \n",
        "        \"eta_min\" : 0.0001,\n",
        "        \"weight_decay\" : 1e-6,\n",
        "        \"max_epochs\" : 20,\n",
        "        \"weight\" : [1.0, 1.0],\n",
        "        #\"patience\" : 8,\n",
        "        }"
      ],
      "metadata": {
        "id": "N8B7UdHx0apH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmK6tKmtMe0i"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\"\n",
        "trainer = Semantic_Seg_Trainer(model=seg_model, opt=\"Adam\", lr=Args[\"lr\"], has_scheduler=True, device=device).to(device)\n",
        "start_time = time.time()\n",
        "trainer.train(train_loader, valid_loader, max_epochs=Args[\"max_epochs\"], disp_epoch=1)\n",
        "print(f\"Training time : {time.time()-start_time:>3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9WeG-kkvZ9q"
      },
      "source": [
        "### Fine-tuning 결과 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVhe8PAblu3P"
      },
      "outputs": [],
      "source": [
        "device=\"cuda\"\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/pth_path/log_epoch_12.pth\"\n",
        "\n",
        "Model = seg_model\n",
        "Model.load_state_dict(torch.load(path))\n",
        "Model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in Model.state_dict():\n",
        "    print(f\"{param_tensor}, \\t {Model.state_dict()[param_tensor].size()}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "k3qA1wj4gHsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_load = Semantic_Seg_Trainer(model=Model, opt=\"adam\", lr=Args[\"lr\"], has_scheduler=False, device=device).to(device)\n",
        "trainer_load.test(test_loader)"
      ],
      "metadata": {
        "id": "4CaJ1AAm50_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv3nlpbJQcvu"
      },
      "outputs": [],
      "source": [
        "test_img = Image.open(os.path.join(root,'test/images/20190409_8362_21709453_41bdb25f9d1107ab02a9327c0232ab59.jpg'))\n",
        "test_mask = Image.open(os.path.join(root,'test/masks/20190409_8362_21709453_41bdb25f9d1107ab02a9327c0232ab59.jpg'))\n",
        "\n",
        "infer_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n",
        "input_image = infer_transform(test_img).to(device)\n",
        "\n",
        "output = Model(input_image.unsqueeze(dim=0))\n",
        "output['out'].shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_img\n",
        "plt.imshow(test_img)"
      ],
      "metadata": {
        "id": "xisnwjX-hw_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mask\n",
        "plt.imshow(test_mask)"
      ],
      "metadata": {
        "id": "cFJI6ZQzhwvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vhUQcaRlu3Q"
      },
      "outputs": [],
      "source": [
        "#prediction\n",
        "cls = torch.argmax(output['out'][0].to(\"cpu\"), dim=0).numpy()\n",
        "out = np.zeros_like(cls)\n",
        "out[cls==1] = 1\n",
        "plt.imshow(out)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LwvCWpoS-7nK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": " Semantic-Segmentation-for-Vehicle-breakage-detection.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}